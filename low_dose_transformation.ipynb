{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset"
      ],
      "metadata": {
        "id": "Bj0Q4ZoEZAQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "7TP7dlYqY8XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Autoencoder Model\n",
        "\n"
      ],
      "metadata": {
        "id": "Ao3ud1iSX_51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dADDkpzrX3QH",
        "outputId": "7b4a5d8f-e99d-4733-866d-fc94997418f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Check if a GPU is available and if not, fall back to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the attention block\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // 8, kernel_size=1),  # Reduce dimensionality\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels // 8, in_channels, kernel_size=1),  # Restore to original channels\n",
        "            nn.Sigmoid()  # Output attention map with values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attention_weights = self.attention(x)\n",
        "        return x * attention_weights  # Apply attention weights to the input feature map\n",
        "\n",
        "\n",
        "# Define the U-Net style autoencoder with attention\n",
        "class UNetStyleAutoencoderWithAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNetStyleAutoencoderWithAttention, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 4, stride=2, padding=1),  # (batch, 64, 256, 256)\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(0.3)  # Add dropout with a probability of 30%\n",
        "        )\n",
        "\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # (batch, 128, 128, 128)\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.3)  # Add dropout\n",
        "        )\n",
        "\n",
        "        # Attention blocks after each encoding layer\n",
        "        self.attention1 = AttentionBlock(64)\n",
        "        self.attention2 = AttentionBlock(128)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),   # (batch, 64, 256, 256)\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(0.3)  # Add dropout\n",
        "        )\n",
        "\n",
        "        self.decoder2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 1, 4, stride=2, padding=1),    # (batch, 1, 512, 512)\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.encoder1(x)   # Output from the first encoder block (batch, 64, 256, 256)\n",
        "        e1 = self.attention1(e1)  # Apply attention block to e1\n",
        "        e2 = self.encoder2(e1)  # Output from the second encoder block (batch, 128, 128, 128)\n",
        "        e2 = self.attention2(e2)  # Apply attention block to e2\n",
        "\n",
        "        # Decoder\n",
        "        d1 = self.decoder1(e2)  # First decoding step (upsampled to batch, 64, 256, 256)\n",
        "        # Concatenate skip connection from e1 (batch, 64, 256, 256)\n",
        "        d1 = torch.cat([d1, e1], dim=1)  # Skip connection with encoder1 output\n",
        "\n",
        "        d2 = self.decoder2(d1)  # Second decoding step (upsampled to batch, 1, 512, 512)\n",
        "\n",
        "        return d2\n",
        "\n",
        "\n",
        "# Instantiate the model and move it to the GPU if available\n",
        "model = UNetStyleAutoencoderWithAttention().to(device)\n",
        "\n",
        "# Example input tensor (batch size of 1, 1 channel, 512x512 image)\n",
        "input_tensor = torch.randn(1, 1, 512, 512).to(device)  # Move input to GPU\n",
        "\n",
        "# Forward pass through the model\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(output.shape)  # Ensure the output is the expected size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset and merge"
      ],
      "metadata": {
        "id": "EBe3Z0Jheijm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the function to load datasets\n",
        "def load_all_datasets(base_path, max_datasets=10):\n",
        "    high_dose_imgs = []\n",
        "    low_dose_imgs = []\n",
        "\n",
        "    # Counter to track how many datasets have been loaded\n",
        "    dataset_count = 0\n",
        "\n",
        "    for file_name in os.listdir(base_path):\n",
        "        if file_name.endswith('_dataset.npz'):\n",
        "            file_path = os.path.join(base_path, file_name)\n",
        "            print(f\"Loading dataset: {file_name}\")\n",
        "\n",
        "            try:\n",
        "                with np.load(file_path) as data:\n",
        "                    high_dose_imgs.append(data['high_dose_img'])\n",
        "                    low_dose_imgs.append(data['low_dose_img'])\n",
        "\n",
        "                    # Increment the count of datasets loaded\n",
        "                    dataset_count += 1\n",
        "\n",
        "                    # Stop loading if we've reached the maximum number of datasets\n",
        "                    if dataset_count >= max_datasets:\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading dataset {file_name}: {e}\")\n",
        "\n",
        "    # Concatenate all images into a global array\n",
        "    high_dose_imgs = np.concatenate(high_dose_imgs, axis=0)\n",
        "    low_dose_imgs = np.concatenate(low_dose_imgs, axis=0)\n",
        "\n",
        "    return high_dose_imgs, low_dose_imgs"
      ],
      "metadata": {
        "id": "eykdR7rleiJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data for training"
      ],
      "metadata": {
        "id": "VjMoGkiMfmA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Function to prepare data\n",
        "def prepare_data(high_dose_imgs, low_dose_imgs, test_size=0.2, batch_size=16):\n",
        "    # Normalize images to [0, 1] range and convert to float16 to reduce memory usage\n",
        "    high_dose_imgs = high_dose_imgs.astype(np.float32) / 255.0\n",
        "    low_dose_imgs = low_dose_imgs.astype(np.float32) / 255.0\n",
        "\n",
        "    # Convert to PyTorch tensors and add channel dimension (N, 1, 512, 512)\n",
        "    high_dose_tensors = torch.from_numpy(high_dose_imgs).unsqueeze(1)\n",
        "    low_dose_tensors = torch.from_numpy(low_dose_imgs).unsqueeze(1)\n",
        "\n",
        "    # Delete the numpy arrays to free memory\n",
        "    del high_dose_imgs\n",
        "    del low_dose_imgs\n",
        "    torch.cuda.empty_cache()  # Clear the CUDA memory cache if using GPU\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    train_high, test_high, train_low, test_low = train_test_split(\n",
        "        high_dose_tensors, low_dose_tensors, test_size=test_size, random_state=42\n",
        "    )\n",
        "\n",
        "    # Clear memory by deleting large tensors after splitting\n",
        "    del high_dose_tensors\n",
        "    del low_dose_tensors\n",
        "    torch.cuda.empty_cache()  # Clear the CUDA memory cache if using GPU\n",
        "\n",
        "    # Create PyTorch datasets\n",
        "    train_dataset = TensorDataset(train_high, train_low)\n",
        "    test_dataset = TensorDataset(test_high, test_low)\n",
        "\n",
        "    # Create DataLoader for batching with pinned memory for faster transfers if using GPU\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=torch.cuda.is_available())\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "    return train_loader, test_loader\n"
      ],
      "metadata": {
        "id": "wim8eNamfo1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Loss Function"
      ],
      "metadata": {
        "id": "WhTrWAFIggSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ssim_loss(predicted, target, window_size=11, size_average=True):\n",
        "    C1 = 0.01 ** 2\n",
        "    C2 = 0.03 ** 2\n",
        "\n",
        "    mu_predicted = F.avg_pool2d(predicted, window_size, 1)\n",
        "    mu_target = F.avg_pool2d(target, window_size, 1)\n",
        "\n",
        "    mu_predicted_sq = mu_predicted.pow(2)\n",
        "    mu_target_sq = mu_target.pow(2)\n",
        "    mu_pred_target = mu_predicted * mu_target\n",
        "\n",
        "    sigma_predicted = F.avg_pool2d(predicted * predicted, window_size, 1) - mu_predicted_sq\n",
        "    sigma_target = F.avg_pool2d(target * target, window_size, 1) - mu_target_sq\n",
        "    sigma_pred_target = F.avg_pool2d(predicted * target, window_size, 1) - mu_pred_target\n",
        "\n",
        "    ssim_numerator = (2 * mu_pred_target + C1) * (2 * sigma_pred_target + C2)\n",
        "    ssim_denominator = (mu_predicted_sq + mu_target_sq + C1) * (sigma_predicted + sigma_target + C2)\n",
        "\n",
        "    ssim_map = ssim_numerator / ssim_denominator\n",
        "    if size_average:\n",
        "        return torch.clamp((1 - ssim_map.mean()) / 2, 0, 1)\n",
        "    else:\n",
        "        return torch.clamp((1 - ssim_map) / 2, 0, 1)\n",
        "\n",
        "def psnr(predicted, target, max_pixel_value=1.0):\n",
        "    \"\"\"\n",
        "    Calculates the Peak Signal-to-Noise Ratio (PSNR) between predicted and target images.\n",
        "\n",
        "    Args:\n",
        "    predicted (torch.Tensor): The predicted (reconstructed) image.\n",
        "    target (torch.Tensor): The ground truth (low-dose) image.\n",
        "    max_pixel_value (float): The maximum possible pixel value in the image (default is 1.0 for normalized images).\n",
        "\n",
        "    Returns:\n",
        "    float: The PSNR value.\n",
        "    \"\"\"\n",
        "    mse = F.mse_loss(predicted, target)\n",
        "    psnr_value = 10 * torch.log10(max_pixel_value ** 2 / mse)\n",
        "    return psnr_value\n",
        "\n",
        "# Example usage within a training loop\n",
        "predicted = torch.rand(1, 1, 256, 256)  # Example reconstructed image\n",
        "target = torch.rand(1, 1, 256, 256)     # Example real low-dose image\n",
        "\n",
        "psnr_value = psnr(predicted, target)\n",
        "print(f\"PSNR: {psnr_value:.2f} dB\")\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.84):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.alpha = alpha  # Weight between SSIM and MAE loss\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        # SSIM Loss\n",
        "        ssim_l = ssim_loss(predicted, target)\n",
        "\n",
        "        # MAE Loss\n",
        "        mae_l = F.mse_loss(predicted, target)\n",
        "\n",
        "        # Combined Loss\n",
        "        loss = self.alpha * ssim_l + (1 - self.alpha) * mae_l\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CombinedLossWithIntensity(nn.Module):\n",
        "    def __init__(self, alpha=0.84, beta=0.1):  # beta for intensity\n",
        "        super(CombinedLossWithIntensity, self).__init__()\n",
        "        self.alpha = alpha  # Weight between SSIM and MAE loss\n",
        "        self.beta = beta  # Weight for intensity loss\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        # SSIM Loss\n",
        "        ssim_l = ssim_loss(predicted, target)\n",
        "\n",
        "        # MAE Loss\n",
        "        mae_l = F.l1_loss(predicted, target)\n",
        "\n",
        "        # Intensity Loss (e.g., mean pixel intensity difference)\n",
        "        intensity_l = torch.abs(predicted.mean() - target.mean())\n",
        "\n",
        "        # Combined Loss\n",
        "        loss = self.alpha * ssim_l + (1 - self.alpha) * mae_l + self.beta * intensity_l\n",
        "        return loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WbWpEsLghWQ",
        "outputId": "42aa28d4-630a-4552-fbf2-625eb2979767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PSNR: 7.77 dB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the results"
      ],
      "metadata": {
        "id": "XJN-okYXg23K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def calculate_psnr(img1, img2):\n",
        "    \"\"\"Calculate PSNR between two images.\"\"\"\n",
        "    return peak_signal_noise_ratio(img1, img2, data_range=img2.max() - img2.min())\n",
        "\n",
        "def calculate_ssim(img1, img2):\n",
        "    \"\"\"Calculate SSIM between two images.\"\"\"\n",
        "    return structural_similarity(img1, img2, data_range=img2.max() - img2.min())\n",
        "\n",
        "def evaluate_metrics_on_dataloader(model, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    psnr_total = 0.0\n",
        "    ssim_total = 0.0\n",
        "    num_samples = len(dataloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for high_dose, low_dose in dataloader:\n",
        "            high_dose = high_dose.to(device)\n",
        "            low_dose = low_dose.to(device)\n",
        "            output = model(high_dose)\n",
        "\n",
        "            # Convert to NumPy arrays for PSNR and SSIM calculations\n",
        "            low_dose_np = low_dose.squeeze().cpu().numpy()\n",
        "            reconstructed_np = output.squeeze().cpu().numpy()\n",
        "\n",
        "            # Calculate PSNR and SSIM\n",
        "            psnr_value = calculate_psnr(low_dose_np, reconstructed_np)\n",
        "            ssim_value = calculate_ssim(low_dose_np, reconstructed_np)\n",
        "\n",
        "            psnr_total += psnr_value\n",
        "            ssim_total += ssim_value\n",
        "\n",
        "    # Calculate average PSNR and SSIM\n",
        "    avg_psnr = psnr_total / num_samples\n",
        "    avg_ssim = ssim_total / num_samples\n",
        "\n",
        "    return avg_psnr, avg_ssim\n",
        "\n",
        "from skimage.exposure import match_histograms\n",
        "\n",
        "def apply_histogram_matching(reconstructed_image, reference_image):\n",
        "    \"\"\"\n",
        "    Apply histogram matching to adjust the reconstructed image to match the intensity\n",
        "    distribution of the reference low-dose image.\n",
        "    \"\"\"\n",
        "    # Perform histogram matching\n",
        "    matched_image = match_histograms(reconstructed_image, reference_image, channel_axis=None)\n",
        "    return matched_image\n",
        "\n",
        "def visualize_results_with_histograms(model, test_loader, save_path='output_figure.png'):\n",
        "    # Evaluate metrics on the entire dataset (before histogram matching)\n",
        "    avg_psnr, avg_ssim = evaluate_metrics_on_dataloader(model, test_loader)\n",
        "    print(f'Average PSNR: {avg_psnr:.2f} dB, Average SSIM: {avg_ssim:.4f}')\n",
        "\n",
        "    # Randomly select a pair of images for visualization\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for high_dose, low_dose in test_loader:\n",
        "            high_dose = high_dose.to(device)\n",
        "            low_dose = low_dose.to(device)\n",
        "            output = model(high_dose)\n",
        "            break  # Only take one example for visualization\n",
        "\n",
        "    # Convert to NumPy arrays for visualization\n",
        "    high_dose_np = high_dose.squeeze().cpu().numpy()\n",
        "    low_dose_np = low_dose.squeeze().cpu().numpy()\n",
        "    reconstructed_np = output.squeeze().cpu().numpy()\n",
        "\n",
        "    # Apply histogram matching to the reconstructed image\n",
        "    matched_reconstructed_np = apply_histogram_matching(reconstructed_np, low_dose_np)\n",
        "\n",
        "    # Now create a new dataset for the histogram-matched images\n",
        "    class MatchedDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, matched_images, high_dose_images):\n",
        "            self.matched_images = matched_images\n",
        "            self.high_dose_images = high_dose_images\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.matched_images)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            # Convert to 4D tensors: (1, 1, H, W) to match (batch_size, channels, height, width)\n",
        "            matched_image = torch.tensor(self.matched_images[idx]).unsqueeze(0)\n",
        "            high_dose_image = torch.tensor(self.high_dose_images[idx]).unsqueeze(0)\n",
        "            return matched_image, high_dose_image\n",
        "\n",
        "    # Create a dataset with the single pair of matched reconstructed and high-dose image\n",
        "    matched_dataset = MatchedDataset([matched_reconstructed_np], [high_dose_np])\n",
        "\n",
        "    # Create a DataLoader for the matched dataset\n",
        "    matched_loader = DataLoader(matched_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Evaluate PSNR and SSIM for the histogram-matched dataset\n",
        "    matched_psnr, matched_ssim = evaluate_metrics_on_dataloader(model, matched_loader)\n",
        "    print(f'After Histogram Matching - PSNR: {matched_psnr:.2f} dB, SSIM: {matched_ssim:.4f}')\n",
        "\n",
        "    # Plot original low-dose, high-dose, reconstructed images, and the matched image\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(18, 6))  # Single row for images\n",
        "\n",
        "    # Display images\n",
        "    axes[0].imshow(high_dose_np, cmap='gray')\n",
        "    axes[0].set_title('Original High-Dose Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(low_dose_np, cmap='gray')\n",
        "    axes[1].set_title('Original Low-Dose Image')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(reconstructed_np, cmap='gray')\n",
        "    axes[2].set_title(f'Reconstructed Low-Dose Image\\nPSNR: {avg_psnr:.2f} dB, SSIM: {avg_ssim:.4f}')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    axes[3].imshow(matched_reconstructed_np, cmap='gray')\n",
        "    axes[3].set_title(f'Histogram-Matched Image\\nPSNR: {matched_psnr:.2f} dB, SSIM: {matched_ssim:.4f}')\n",
        "    axes[3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure as PNG\n",
        "    plt.savefig(save_path, format='png')\n",
        "\n",
        "    # Show the figure\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Yf6nfP7Fg4cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Plot randomly"
      ],
      "metadata": {
        "id": "SpZgHTiD38qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr_metric, structural_similarity as ssim_metric\n",
        "from torch.utils.data import DataLoader\n",
        "from skimage.exposure import match_histograms\n",
        "import random\n",
        "\n",
        "def calculate_psnr(img1, img2):\n",
        "    \"\"\"Calculate PSNR between two images.\"\"\"\n",
        "    return psnr_metric(img1, img2, data_range=img2.max() - img2.min())\n",
        "\n",
        "def calculate_ssim(img1, img2):\n",
        "    \"\"\"Calculate SSIM between two images.\"\"\"\n",
        "    return ssim_metric(img1, img2, data_range=img2.max() - img2.min())\n",
        "\n",
        "def apply_histogram_matching(reconstructed_image, reference_image):\n",
        "    \"\"\"\n",
        "    Apply histogram matching to adjust the reconstructed image to match the intensity\n",
        "    distribution of the reference low-dose image.\n",
        "    \"\"\"\n",
        "    # Perform histogram matching\n",
        "    matched_image = match_histograms(reconstructed_image, reference_image, channel_axis=None)\n",
        "    return matched_image\n",
        "\n",
        "def visualize_single_pair_with_histograms(model, test_loader, save_path='output_figure_single.png'):\n",
        "    # Randomly select a pair of images for visualization\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for high_dose, low_dose in test_loader:\n",
        "            high_dose = high_dose.to(device)\n",
        "            low_dose = low_dose.to(device)\n",
        "            output = model(low_dose)\n",
        "\n",
        "            # Randomly select an index from the current batch\n",
        "            random_idx = random.randint(0, high_dose.size(0) - 1)\n",
        "\n",
        "\n",
        "            break  # Only take one example for visualization\n",
        "\n",
        "    # Select only the first image from the batch\n",
        "    high_dose_np = high_dose[random_idx].squeeze().cpu().numpy()\n",
        "    low_dose_np = low_dose[random_idx].squeeze().cpu().numpy()\n",
        "    reconstructed_np = output[random_idx].squeeze().cpu().numpy()\n",
        "\n",
        "    # Calculate PSNR and SSIM for the original reconstructed image\n",
        "    psnr_value = calculate_psnr(low_dose_np, reconstructed_np)\n",
        "    ssim_value = calculate_ssim(low_dose_np, reconstructed_np)\n",
        "\n",
        "    print(f'Before Histogram Matching - PSNR: {psnr_value:.2f} dB, SSIM: {ssim_value:.4f}')\n",
        "\n",
        "\n",
        "    # Plot original low-dose, high-dose, reconstructed images, and the matched image\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # Single row for images\n",
        "\n",
        "    # Display images\n",
        "    axes[0].imshow(high_dose_np, cmap='gray')\n",
        "    axes[0].set_title('Original High-Dose Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(low_dose_np, cmap='gray')\n",
        "    axes[1].set_title('Original Low-Dose Image')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(reconstructed_np, cmap='gray')\n",
        "    axes[2].set_title(f'Reconstructed Low-Dose Image\\nPSNR: {psnr_value:.2f} dB, SSIM: {ssim_value:.4f}')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure as PNG\n",
        "    plt.savefig(save_path, format='png')\n",
        "\n",
        "    # Show the figure\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "TRrg2LMv3_6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main"
      ],
      "metadata": {
        "id": "2gKX-yrYgSiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the base path to the sample_data directory in Colab\n",
        "if __name__ == \"__main__\":\n",
        "    base_path = '/content/drive/My Drive/Thesis Master/dataset' # Path where the .npz files are stored\n",
        "\n",
        "    # Load and prepare data\n",
        "    high_dose_imgs, low_dose_imgs = load_all_datasets(base_path, max_datasets=49)\n",
        "\n",
        "    # Check the shape of the loaded data\n",
        "    print(f\"High Dose Images Shape: {high_dose_imgs.shape}\")\n",
        "    print(f\"Low Dose Images Shape: {low_dose_imgs.shape}\")\n",
        "\n",
        "    # Assuming high_dose_imgs and low_dose_imgs are already loaded\n",
        "    #train_loader, test_loader = prepare_data(high_dose_imgs, low_dose_imgs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQi9ZybogUW2",
        "outputId": "3f549dfb-30e0-41dc-fa33-32cefcfc451a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset: C002_dataset.npz\n",
            "Loading dataset: C004_dataset.npz\n",
            "Loading dataset: C012_dataset.npz\n",
            "Loading dataset: C016_dataset.npz\n",
            "Loading dataset: C021_dataset.npz\n",
            "Loading dataset: C027_dataset.npz\n",
            "Loading dataset: C030_dataset.npz\n",
            "Loading dataset: C050_dataset.npz\n",
            "Loading dataset: C052_dataset.npz\n",
            "Loading dataset: C067_dataset.npz\n",
            "Loading dataset: C077_dataset.npz\n",
            "Loading dataset: C081_dataset.npz\n",
            "Loading dataset: C095_dataset.npz\n",
            "Loading dataset: C099_dataset.npz\n",
            "Loading dataset: C107_dataset.npz\n",
            "Loading dataset: C111_dataset.npz\n",
            "Loading dataset: C120_dataset.npz\n",
            "Loading dataset: C121_dataset.npz\n",
            "Loading dataset: C124_dataset.npz\n",
            "Loading dataset: C128_dataset.npz\n",
            "Loading dataset: C130_dataset.npz\n",
            "Loading dataset: C135_dataset.npz\n",
            "Loading dataset: C158_dataset.npz\n",
            "Loading dataset: C160_dataset.npz\n",
            "Loading dataset: C162_dataset.npz\n",
            "Loading dataset: C166_dataset.npz\n",
            "Loading dataset: C170_dataset.npz\n",
            "Loading dataset: C179_dataset.npz\n",
            "Loading dataset: C190_dataset.npz\n",
            "Loading dataset: C193_dataset.npz\n",
            "Loading dataset: C202_dataset.npz\n",
            "Loading dataset: C203_dataset.npz\n",
            "Loading dataset: C218_dataset.npz\n",
            "Loading dataset: C219_dataset.npz\n",
            "Loading dataset: C224_dataset.npz\n",
            "Loading dataset: C227_dataset.npz\n",
            "Loading dataset: C232_dataset.npz\n",
            "Loading dataset: C234_dataset.npz\n",
            "Loading dataset: C241_dataset.npz\n",
            "Loading dataset: C246_dataset.npz\n",
            "Loading dataset: C249_dataset.npz\n",
            "Loading dataset: C252_dataset.npz\n",
            "Loading dataset: C257_dataset.npz\n",
            "Loading dataset: C258_dataset.npz\n",
            "Loading dataset: C261_dataset.npz\n",
            "Loading dataset: C267_dataset.npz\n",
            "Loading dataset: C268_dataset.npz\n",
            "Loading dataset: C280_dataset.npz\n",
            "Loading dataset: C295_dataset.npz\n",
            "High Dose Images Shape: (16984, 512, 512)\n",
            "Low Dose Images Shape: (16984, 512, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = prepare_data(high_dose_imgs, low_dose_imgs)"
      ],
      "metadata": {
        "id": "rAAKC_ucrCxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wQcQ75RSa3Nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "wMicT34gixRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import pytorch_ssim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = UNetStyleAutoencoderWithAttention().to(device)  # Move model to GPU\n",
        "# criterion = nn.L1Loss()  # If using L1 loss as an alternative\n",
        "loss_fn = CombinedLossWithIntensity(alpha=0.7, beta=0.1)  # Assuming this is your custom loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 4\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for high_dose, low_dose in train_loader:\n",
        "        # Move the data to the GPU\n",
        "        high_dose = high_dose.to(device)\n",
        "        low_dose = low_dose.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(high_dose)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(outputs, low_dose)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Clipping to avoid exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "        running_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "HTu1RBtqizug",
        "outputId": "4c6fc111-b20c-49aa-b0f7-a9efb8fa3ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b41c66d96d4e>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_dose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c85894ff7b35>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0me1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Output from the first encoder block (batch, 64, 256, 256)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0me1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply attention block to e1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0me2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Output from the second encoder block (batch, 128, 128, 128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0me2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply attention block to e2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DE2gjQUXJ63t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1D5ky_Gq45fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_single_pair_with_histograms(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "GGG2k57cjfIg",
        "outputId": "22c52153-4854-4b64-e0a1-0ffe6e4b8463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'visualize_single_pair_with_histograms' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-12f56ebd32e1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize_single_pair_with_histograms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'visualize_single_pair_with_histograms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "! pip install pytorch_ssim"
      ],
      "metadata": {
        "id": "tHcP88NEa1FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pytorch_ssim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHiJiNESa3y5",
        "outputId": "66456e10-a1a5-435f-f002-66133915c17b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_ssim\n",
            "  Downloading pytorch_ssim-0.1.tar.gz (1.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch_ssim\n",
            "  Building wheel for pytorch_ssim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch_ssim: filename=pytorch_ssim-0.1-py3-none-any.whl size=2007 sha256=e4f8faef4b55e07f7e0d7e99c6aeebb3a2de01a7b8bff3b328b39e412b6dcda6\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/0c/10/4a3f91bd610b23196f1e28f8af80b3ec86786b50f3e86dc21e\n",
            "Successfully built pytorch_ssim\n",
            "Installing collected packages: pytorch_ssim\n",
            "Successfully installed pytorch_ssim-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5PzSgtZkX-xE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxDdjFDpojT9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}