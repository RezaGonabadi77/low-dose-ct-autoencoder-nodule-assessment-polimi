{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation complete. Saved 1470 new masks with corresponding CT volumes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nrrd\n",
    "import cv2\n",
    "from scipy.ndimage import binary_dilation, binary_erosion\n",
    "from skimage.util import random_noise\n",
    "from skimage.segmentation import slic\n",
    "from skimage.draw import polygon\n",
    "\n",
    "# Load the metadata file\n",
    "metadata_path = \"C:/Reza Gonabadi/Polimi/Master Thesis/LIDC-IDRI_Simplified_low_asus_Output/all_nodules_metadata.csv\"\n",
    "df = pd.read_csv(metadata_path)\n",
    "\n",
    "# Filter relevant masks (Consensus + Moderately/Highly Suspicious)\n",
    "target_classes = [\"Malignant\"]\n",
    "filtered_df = df[(df['Mask'].str.contains('consensus')) & (df['Malignancy'].isin(target_classes))]\n",
    "\n",
    "# Target balance (aim for 1600 samples)\n",
    "current_count = len(filtered_df)\n",
    "target_count = 1600\n",
    "augmentation_factor = target_count // current_count\n",
    "\n",
    "# Function to perform augmentations\n",
    "def augment_mask(mask):\n",
    "    augmented_masks = []\n",
    "    \n",
    "    # Dilation with factor 0.15\n",
    "    dilated = binary_dilation(mask, iterations=int(0.15 * mask.shape[0])).astype(mask.dtype)\n",
    "    augmented_masks.append(dilated)\n",
    "    \n",
    "    # Erosion with factor -0.15\n",
    "    eroded = binary_erosion(mask, iterations=int(abs(-0.15) * mask.shape[0])).astype(mask.dtype)\n",
    "    augmented_masks.append(eroded)\n",
    "    \n",
    "    # Random Noise\n",
    "    noisy = random_noise(mask, mode='s&p', amount=0.05)\n",
    "    noisy = (noisy > 0.5).astype(mask.dtype)\n",
    "    augmented_masks.append(noisy)\n",
    "    \n",
    "    # Contour Randomization using SLIC superpixels, repeated twice\n",
    "    for _ in range(2):\n",
    "        segments = slic(mask, n_segments=50, compactness=10, sigma=1)\n",
    "        perturbed_mask = mask.copy()\n",
    "        for segment in np.unique(segments):\n",
    "            coords = np.column_stack(np.where(segments == segment))\n",
    "            if np.random.rand() > 0.5:  # Randomly perturb some segments\n",
    "                rr, cc = polygon(coords[:, 0], coords[:, 1], mask.shape)\n",
    "                perturbed_mask[rr, cc] = np.random.choice([0, 1])\n",
    "        augmented_masks.append(perturbed_mask)\n",
    "    \n",
    "    return augmented_masks\n",
    "\n",
    "# Directory for augmented masks\n",
    "output_dir = \"C:/Reza Gonabadi/Polimi/Master Thesis/LIDC-IDRI_Simplified_low_asus_Output/Augmented\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each mask\n",
    "new_entries = []\n",
    "for _, row in filtered_df.iterrows():\n",
    "    mask_path = row['Mask']\n",
    "    image_path = row['Image']  # Get the corresponding CT volume\n",
    "    patient_id, nodule_id = row['Patient_ID'], row['Nodule_ID']\n",
    "    \n",
    "    # Load NRRD mask\n",
    "    data, header = nrrd.read(mask_path)\n",
    "    \n",
    "    # Apply augmentations\n",
    "    augmented_masks = augment_mask(data)\n",
    "    \n",
    "    # Save augmented masks\n",
    "    for i, aug_mask in enumerate(augmented_masks):\n",
    "        new_filename = f\"{patient_id}_nodule_{nodule_id}_aug_{i}.nrrd\"\n",
    "        save_path = os.path.join(output_dir, new_filename)\n",
    "        nrrd.write(save_path, aug_mask, header)\n",
    "        \n",
    "        # Add to new metadata entries with the same image path\n",
    "        new_entries.append([patient_id, nodule_id, image_path, save_path, row['Malignancy']])\n",
    "\n",
    "# Convert new entries to DataFrame\n",
    "augmented_df = pd.DataFrame(new_entries, columns=['Patient_ID', 'Nodule_ID', 'Image', 'Mask', 'Malignancy'])\n",
    "\n",
    "# Merge with original metadata\n",
    "df_final = pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "# Save updated metadata\n",
    "updated_metadata_path = \"C:/Reza Gonabadi/Polimi/Master Thesis/LIDC-IDRI_Simplified_low_asus_Output/all_nodules_metadata_augmented1.csv\"\n",
    "df_final = df_final[df_final['Malignancy'].isin(target_classes)]  # Ensure only target classes remain\n",
    "df_final.to_csv(updated_metadata_path, index=False)\n",
    "\n",
    "print(f\"Augmentation complete. Saved {len(new_entries)} new masks with corresponding CT volumes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1470 augmented masks for feature extraction.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load augmented metadata\n",
    "metadata_path = \"C:/Reza Gonabadi/Polimi/Master Thesis/LIDC-IDRI_Simplified_low_asus_Output/all_nodules_metadata_augmented1.csv\"\n",
    "df = pd.read_csv(metadata_path)\n",
    "\n",
    "# Keep only augmented masks (those containing '_aug_' in filename)\n",
    "df_augmented = df[df['Mask'].str.contains('_aug_')]\n",
    "\n",
    "# Save a new metadata CSV for PyRadiomics\n",
    "augmented_metadata_path = \"C:/Reza Gonabadi/Polimi/Master Thesis/LIDC-IDRI_Simplified_low_asus_Output/all_nodules_metadata_augmented_only.csv\"\n",
    "df_augmented.to_csv(augmented_metadata_path, index=False)\n",
    "\n",
    "print(f\"Filtered {len(df_augmented)} augmented masks for feature extraction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the following command in your terminal to extract radiomics features from augmented masks:\n",
      "\n",
      "pyradiomics \"C:/Reza Gonabadi/Polimi/Master Thesis/LIDC-IDRI_Simplified_low_asus_Output/all_nodules_metadata_augmented_only.csv\" -o \"C:/Reza Gonabadi/Polimi/Master Thesis/LIDC-IDRI_Simplified_low_asus_Output\\all_nodules_metadata_augmented_only.csv\" -f csv --param \"C:/Reza Gonabadi/Polimi/Master Thesis/LIDC-IDRI_Simplified_low_asus_Output\\CT.yaml\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pyradiomics_command = f\"\"\"\n",
    "pyradiomics \"{augmented_metadata_path}\" -o \"{os.path.join(os.path.dirname(augmented_metadata_path), 'all_nodules_metadata_augmented_only.csv')}\" -f csv --param \"{os.path.join(os.path.dirname(augmented_metadata_path), 'CT.yaml')}\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"Run the following command in your terminal to extract radiomics features from augmented masks:\")\n",
    "print(pyradiomics_command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imageFilepath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage exists:\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[43mimageFilepath\u001b[49m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask exists:\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(maskFilepath))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imageFilepath' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Image exists:\", os.path.exists(imageFilepath))\n",
    "print(\"Mask exists:\", os.path.exists(maskFilepath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "9 columns passed, passed data had 5 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Reza Gonabadi\\Polimi\\Master Thesis\\python-new-env\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Reza Gonabadi\\Polimi\\Master Thesis\\python-new-env\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 9 columns passed, passed data had 5 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: No valid augmentations were created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 111\u001b[0m     \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 103\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Save updated metadata\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_entries:\n\u001b[1;32m--> 103\u001b[0m     augmented_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_entries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     df_final \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, augmented_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    105\u001b[0m     df_final\u001b[38;5;241m.\u001b[39mto_csv(metadata_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_augmented.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Reza Gonabadi\\Polimi\\Master Thesis\\python-new-env\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m         arrays,\n\u001b[0;32m    861\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m     )\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Reza Gonabadi\\Polimi\\Master Thesis\\python-new-env\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Reza Gonabadi\\Polimi\\Master Thesis\\python-new-env\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32mc:\\Reza Gonabadi\\Polimi\\Master Thesis\\python-new-env\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 9 columns passed, passed data had 5 columns"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nrrd\n",
    "from scipy.ndimage import binary_dilation, binary_erosion\n",
    "from skimage.util import random_noise\n",
    "from skimage.segmentation import slic\n",
    "from skimage.draw import polygon\n",
    "\n",
    "def augment_mask(mask):\n",
    "    \"\"\"Generate augmented versions of a binary mask while ensuring PyRadiomics compatibility\"\"\"\n",
    "    augmented_masks = []\n",
    "    \n",
    "    # 1. Convert to strict binary (0 or 1) with uint8 type\n",
    "    binary_mask = np.where(mask > 0.5, 1, 0).astype(np.uint8)\n",
    "    \n",
    "    # Skip if mask is empty\n",
    "    if np.sum(binary_mask) == 0:\n",
    "        return augmented_masks\n",
    "    \n",
    "    # 2. Basic augmentations (dilation/erosion/noise)\n",
    "    # Dilation - always safe\n",
    "    dilated = binary_dilation(binary_mask, iterations=max(1, int(0.15 * mask.shape[0])))\n",
    "    augmented_masks.append(dilated.astype(np.uint8))\n",
    "    \n",
    "    # Erosion - only keep if substantial\n",
    "    eroded = binary_erosion(binary_mask, iterations=max(1, int(0.15 * mask.shape[0])))\n",
    "    if np.sum(eroded) > 5:  # Minimum 5 voxels\n",
    "        augmented_masks.append(eroded.astype(np.uint8))\n",
    "    \n",
    "    # Noise - threshold to maintain binary\n",
    "    noisy = (random_noise(binary_mask, mode='s&p', amount=0.05) > 0.5).astype(np.uint8)\n",
    "    if np.sum(noisy) > 5:\n",
    "        augmented_masks.append(noisy)\n",
    "    \n",
    "    # 3. Advanced augmentations (SLIC-based)\n",
    "    for _ in range(2):  # Create 2 SLIC variants\n",
    "        try:\n",
    "            # Adaptive segmentation based on mask size\n",
    "            n_segments = max(2, min(50, np.sum(binary_mask)//20))\n",
    "            segments = slic(binary_mask, n_segments=n_segments, compactness=10, sigma=1)\n",
    "            \n",
    "            perturbed = binary_mask.copy()\n",
    "            modified = False\n",
    "            \n",
    "            # Only modify segments containing mask pixels\n",
    "            for seg_val in np.unique(segments[binary_mask > 0]):\n",
    "                if np.random.rand() > 0.6:  # 40% modification probability\n",
    "                    coords = np.where(segments == seg_val)\n",
    "                    if len(coords[0]) > 0:\n",
    "                        rr, cc = polygon(coords[0], coords[1], mask.shape)\n",
    "                        perturbed[rr, cc] = np.random.choice([0, 1])\n",
    "                        modified = True\n",
    "            \n",
    "            if modified and np.sum(perturbed) > 5:\n",
    "                augmented_masks.append(perturbed.astype(np.uint8))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return augmented_masks\n",
    "\n",
    "def process_dataset():\n",
    "    # Load metadata\n",
    "    metadata_path = \"C:/Reza Gonabadi/Polimi/Master Thesis/LIDC-IDRI_Simplified_low_asus_Output/all_nodules_metadata.csv\"\n",
    "    df = pd.read_csv(metadata_path)\n",
    "    \n",
    "    # Filter malignant nodules\n",
    "    target_classes = [\"Malignant\"]\n",
    "    filtered_df = df[(df['Mask'].str.contains('consensus')) & \n",
    "                    (df['Malignancy'].isin(target_classes))]\n",
    "    \n",
    "    # Prepare output\n",
    "    output_dir = \"C:/Reza Gonabadi/Polimi/Master Thesis/LIDC-IDRI_Simplified_low_asus_Output/Augmented\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    new_entries = []\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        try:\n",
    "            # Load original mask\n",
    "            mask, header = nrrd.read(row['Mask'])\n",
    "            \n",
    "            # Generate augmentations\n",
    "            augmented_masks = augment_mask(mask)\n",
    "            \n",
    "            # Save valid augmentations\n",
    "            for i, aug_mask in enumerate(augmented_masks):\n",
    "                if np.sum(aug_mask) > 5:  # Final validation\n",
    "                    new_path = os.path.join(output_dir, \n",
    "                                          f\"{row['Patient_ID']}_nodule_{row['Nodule_ID']}_aug_{i}.nrrd\")\n",
    "                    nrrd.write(new_path, aug_mask, header)\n",
    "                    new_entries.append([\n",
    "                        row['Patient_ID'],\n",
    "                        row['Nodule_ID'],\n",
    "                        row['Image'],\n",
    "                        new_path,\n",
    "                        row['Malignancy']\n",
    "                    ])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['Mask']}: {str(e)}\")\n",
    "    \n",
    "    # Save updated metadata\n",
    "    if new_entries:\n",
    "        augmented_df = pd.DataFrame(new_entries, columns=df.columns)\n",
    "        df_final = pd.concat([df, augmented_df], ignore_index=True)\n",
    "        df_final.to_csv(metadata_path.replace('.csv', '_augmented.csv'), index=False)\n",
    "        print(f\"Created {len(new_entries)} valid augmented masks\")\n",
    "    else:\n",
    "        print(\"Warning: No valid augmentations were created\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
